# PoseLab Starter Pack

Welcome! This Starter Pack contains everything you need to dive deep into pose-based exercise analysis.

---

## ğŸ“¦ What's Inside

### ğŸ® **New Demo** (Start Here!)
**`PoseLab_Demo.ipynb`** - Modern, fast demo using MediaPipe.
- Runs in your browser (Google Colab)
- No GPU needed
- See ROM dials instantly
- Upload any video and get instant analysis

### ğŸ”¬ **Legacy Pipeline**
**`legacy_xgb_pipeline/`** - Original OpenPose + XGBoost approach.
- Full training pipeline (00-03 notebooks)
- 86% accuracy models
- Feature engineering with cesium library
- Complete preprocessing workflow

### ğŸ“Š **Your Data Asset**
**`data/`** - 100+ processed videos across 5 exercises.
- Keypoint trajectories (X, Y coordinates)
- Joint angle time-series
- Ready for your analysis
- Organized by exercise type:
  - Burpees
  - Squats
  - Pushups
  - Jumping Jacks
  - Mountain Climbers

### ğŸ¤– **Trained Models**
**`models/xgb/`** - 8 pre-trained XGBoost classifiers.
- One model per joint (L/R Elbow, Shoulder, Hip, Knee)
- Load and predict immediately
- JSON and pickle formats

---

## ğŸš€ Quick Start Guide

### Option 1: Try the Modern Demo (Recommended First!)

1. **Open Google Colab**: Go to [colab.research.google.com](https://colab.research.google.com)
2. **Upload** `PoseLab_Demo.ipynb`
3. **Run all cells** â†’ See your ROM analysis in minutes!

### Option 2: Explore the Legacy Pipeline

1. **Open** `legacy_xgb_pipeline/00_Apply_OpenPose_to_Raw_Video.ipynb`
2. **Run through sequentially**: 00 â†’ 01 â†’ 02 â†’ 03
3. **Train models**: Use your own data or ours

### Option 3: Use Pre-trained Models

```python
import pickle
import pandas as pd

# Load a model
with open('models/xgb/L_ELB_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Load your joint angle data
df = pd.read_csv('data/squats/squats_01_joint_angles.csv')

# Predict
# (You'll need to extract features first - see 03_XGB notebook)
```

---

## ğŸ”¬ The Science Behind It

### The Challenge
2D pose estimation from video is inherently noisy. Traditional 3D reconstruction methods fail because "the accumulated errors cannot be used to extract any sort of useful information when compared to 3D metrics."

### The Innovation
**Don't solve 3D. Learn movement patterns.**

This approach calculates **temporal features** from joint angles:
- **Amplitude**: Range of motion
- **Max slope**: Speed of movement
- **Median absolute deviation**: Consistency
- **Skewness**: Asymmetry
- **Weighted average**: Overall pattern
- And 6 more features...

### Why It Works
The XGBoost models don't try to guess 3D positions. Instead, they learn **how joints move together over time**. This makes them robust to:
- Camera angle variations
- Lighting conditions
- 2D projection noise
- Individual body differences

**Result**: 86% accuracy across 5 exercise types!

---

## ğŸ“‚ Directory Structure

```
PoseLab_Starter_Pack/
â”‚
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ LICENSE.txt                         # License information
â”‚
â”œâ”€â”€ PoseLab_Demo.ipynb                  # ğŸ® NEW: Modern demo (MediaPipe)
â”‚
â”œâ”€â”€ legacy_xgb_pipeline/                # ğŸ”¬ Original research code
â”‚   â”œâ”€â”€ 00_Apply_OpenPose_to_Raw_Video.ipynb
â”‚   â”œâ”€â”€ 00_apply_openpose_to_raw_video.py
â”‚   â”œâ”€â”€ 01_Apply_Preprocessing_steps.ipynb
â”‚   â”œâ”€â”€ 01_apply_preprocessing_steps_to_raw_data_and_save_plots.py
â”‚   â”œâ”€â”€ 02_Calculate_Joint_Angles.ipynb
â”‚   â”œâ”€â”€ 02_calculate_joint_angles.py
â”‚   â”œâ”€â”€ 03_XGB_Feature_Generation_and_Model_Creation.ipynb
â”‚   â”œâ”€â”€ 03_xgb_feature_generation_and_model_creation.py
â”‚   â”œâ”€â”€ 03_XGB_Model_Testing.ipynb
â”‚   â””â”€â”€ 03_xgb_model_testing.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ xgb/                            # ğŸ¤– Pre-trained models
â”‚       â”œâ”€â”€ L_ELB_model.json
â”‚       â”œâ”€â”€ L_ELB_model.pkl
â”‚       â”œâ”€â”€ R_ELB_model.json
â”‚       â”œâ”€â”€ R_ELB_model.pkl
â”‚       â”œâ”€â”€ L_SHO_model.json
â”‚       â”œâ”€â”€ L_SHO_model.pkl
â”‚       â”œâ”€â”€ R_SHO_model.json
â”‚       â”œâ”€â”€ R_SHO_model.pkl
â”‚       â”œâ”€â”€ L_HIP_model.json
â”‚       â”œâ”€â”€ L_HIP_model.pkl
â”‚       â”œâ”€â”€ R_HIP_model.json
â”‚       â”œâ”€â”€ R_HIP_model.pkl
â”‚       â”œâ”€â”€ L_KNE_model.json
â”‚       â”œâ”€â”€ L_KNE_model.pkl
â”‚       â”œâ”€â”€ R_KNE_model.json
â”‚       â””â”€â”€ R_KNE_model.pkl
â”‚
â””â”€â”€ data/                               # ğŸ“Š Your processed dataset
    â”œâ”€â”€ burpees/
    â”‚   â”œâ”€â”€ burpees_01_joint_angles.csv
    â”‚   â”œâ”€â”€ burpees_02_joint_angles.csv
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ squats/
    â”œâ”€â”€ pushups/
    â”œâ”€â”€ jumping_jacks/
    â””â”€â”€ mountain_climbers/
```

---

## ğŸ’¡ Next Steps

### Build Your Own Classifier
1. Load data from `data/` folder
2. Follow `legacy_xgb_pipeline/03_XGB_Feature_Generation.ipynb`
3. Extract features using cesium
4. Train XGBoost models
5. Evaluate accuracy

### Add New Exercises
1. Record videos of new exercises
2. Run through 00-02 pipeline
3. Generate joint angle data
4. Add to your training set
5. Retrain models

### Create Custom Visualizations
1. Use joint angle data from `data/`
2. Create your own ROM plots
3. Compare exercises
4. Analyze movement patterns

### Use in Research
- Cite the methodology
- Reference the dataset
- Build on the approach
- Contribute back!

---

## ğŸ› Troubleshooting

### "Module not found" errors
```bash
pip install pandas numpy matplotlib scipy cesium xgboost opencv-python mediapipe
```

### Running OpenPose locally
The legacy pipeline uses OpenPose. For local setup:
1. Install CMake, CUDA, cuDNN
2. Clone OpenPose repository
3. Build from source
4. Or use the GPU-enabled Google Colab version

### Model loading issues
Models are in both JSON and pickle formats. Use:
```python
import pickle
model = pickle.load(open('model.pkl', 'rb'))
```

---

## ğŸ“š Additional Resources

### Papers Referenced
- [Computer Vision and Pose Estimation - Paper #1](https://arxiv.org/pdf/2005.03194.pdf)
- [Workout Type Recognition and Repetition Counting](https://www.researchgate.net/publication/333625301_Workout_Type_Recognition_and_Repetition_Counting_with_CNNs_from_3D_Acceleration_Sensed_on_the_Chest)
- [Automatic Recognition of Physical Exercises](https://eprints.leedsbeckett.ac.uk/id/eprint/5932/1/AutomaticRecognitionofPhysicalExercisesPerformedbyStrokeSurvivorstoImproveRemoteRehabilitationAM-MONEKOSSO.pdf)

### Useful Links
- [MediaPipe Documentation](https://google.github.io/mediapipe/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Cesium Library](https://cesium-ml.org/)
- [OpenPose GitHub](https://github.com/CMU-Perceptual-Computing-Lab/openpose)

---

## ğŸ¤ Support

**Need help?**
- Open an issue on GitHub
- Check existing discussions
- Read the code comments

**Found a bug?**
- Report it with steps to reproduce
- Include error messages
- Share your environment details

**Want to contribute?**
- Submit pull requests
- Improve documentation
- Add new features

---

## ğŸ“„ License

See LICENSE.txt for complete licensing information.

---

## ğŸ™ Acknowledgments

Thanks to everyone who has starred, forked, and contributed to this project!

**Special thanks:**
- CMU Perceptual Computing Lab (OpenPose)
- Google MediaPipe team
- The biomechanics research community

---

**Made with â¤ï¸ for researchers, students, and developers pushing the boundaries of human motion analysis.**

*Happy analyzing!* ğŸ¯

