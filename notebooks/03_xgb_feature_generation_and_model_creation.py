# -*- coding: utf-8 -*-
"""03 - XGB - Feature Generation and Model Creation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xJZkwMMnv4tXQcCn3tGl55nny3D4TmS3

### Mount Drive
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""### Install libraries"""

!pip install cesium

import os
import cesium
import xgboost as xgb
import numpy as np
import pandas as pd

from cesium import featurize as ft
import matplotlib.pyplot as plt

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance

PROJ_WORKOUTS_ROOT = os.path.join('/content/gdrive/MyDrive/ColabNotebooks/BiomechanicsAnalysis/___WORKOUTS/data/processed')

workout_types = os.listdir(PROJ_WORKOUTS_ROOT)

data_dict = {
    'burpees':[],
    'jumping_jacks':[],
    'mountain_climbers':[],
    'pushups':[],
    'squats':[]
}

joint_angles_list = ['L_ELB',	'R_ELB',
                     'L_SHO','R_SHO',
                     'L_HIP','R_HIP',
                     'L_KNE','R_KNE']

"""### Load in Dataframes"""

for workout_type in workout_types:
    workout_samples = os.listdir(os.path.join(PROJ_WORKOUTS_ROOT, workout_type)) 
    for sample in workout_samples:
        sample_csv_path = os.path.join(PROJ_WORKOUTS_ROOT, workout_type, sample, f'{sample}_joint_angles.csv')

        df = pd.read_csv(sample_csv_path)
        data_dict[workout_type].append(df)

"""### Convert Joint Angle Data Vector into Numpy Array"""

JA_df_dict = {}

for joint_angle in joint_angles_list:

    dataframe_lst = []

    for key, value_lst in data_dict.items():
        for df in value_lst:
            dataframe_lst.append(
                {   
                'times' : df.index.to_numpy(), 
                'values' : df[joint_angle].to_numpy(), 
                'classes' : key
                }
            )

    JA_df_dict[joint_angle] = pd.DataFrame(dataframe_lst)

"""### Get rid of rows with values of size 0

Looks like only index 86 has a value and time of 0... we will drop it
"""

for ind in JA_df_dict['L_ELB'].index[84:89]:
    print(f"Index:{ind}, {np.size(JA_df_dict['L_ELB']['times'][ind]):<10}, {np.size(JA_df_dict['L_ELB']['values'][ind]):<10}, {np.size(JA_df_dict['L_ELB']['classes'][ind]):<10}")

for key in JA_df_dict.keys():
    JA_df_dict[key] = JA_df_dict[key].drop([86])

for ind in JA_df_dict['L_ELB'].index[84:89]:
    print(f"Index:{ind}, {np.size(JA_df_dict['L_ELB']['times'][ind]):<10}, {np.size(JA_df_dict['L_ELB']['values'][ind]):<10}, {np.size(JA_df_dict['L_ELB']['classes'][ind]):<10}")

"""### Convert all Dataframes to Dicts  (this is the format cesium likes for feature generation)"""

for key in JA_df_dict.keys():

    JA_df_dict[key] = JA_df_dict[key].to_dict()

    times = [value for value in JA_df_dict[key]['times'].values()]
    values = [value for value in JA_df_dict[key]['values'].values()]
    classes = np.array([value for value in JA_df_dict[key]['classes'].values()])


    JA_df_dict[key]['times'] = times
    JA_df_dict[key]['values'] = values
    JA_df_dict[key]['classes'] = classes

"""### Create a dict of features for each joint angle time series set"""

features_to_use = [    
    "amplitude",
    "percent_beyond_1_std",
    "maximum",
    "max_slope",
    "median",
    "median_absolute_deviation",
    "percent_close_to_median",
    "minimum",
    "skew",
    "std",
    "weighted_average"]

features_df_dict = {}

for key in JA_df_dict.keys():

    features_df_dict[key] = ft.featurize_time_series(times=JA_df_dict[key]['times'],
                                              values=JA_df_dict[key]["values"],
                                              errors=None,
                                              features_to_use=features_to_use,
                                              scheduler=None)
    
    features_df_dict[key]['classes'] = JA_df_dict[key]['classes']
    features_df_dict[key].columns =  features_df_dict[key].columns.droplevel(-1)

features_df_dict['L_ELB'].head(3)

"""### Train XGB Model"""

joint_name = []
training_score = []
testing_score = []


feature_importance = []

joint_model = {}

def model_training(df, joint):

    X_train, X_test, y_train, y_test = train_test_split(
        df.iloc[:, :11].values, df["classes"], test_size=0.2)
    
    model = xgb.XGBClassifier(n_estimators=25, max_depth=25) #random_state=16                  
    model.fit(X_train, y_train)

    joint_model[joint] = model

    perm_importance = permutation_importance(model, X_test, y_test)

    joint_name.append(joint)
    training_score.append(model.score(X_train, y_train))
    testing_score.append(model.score(X_test, y_test))

    feature_importance.append(perm_importance)
    # feature_importance.append(perm_importance.importances_mean.argsort())

    # print(f"Joint:{joint:<15}, Train_Score:{model.score(X_train, y_train):<15}, Test_Score:{model.score(X_test, y_test):<15}")


for key in JA_df_dict.keys():
    model_training(features_df_dict[key], key)


pd.options.display.float_format = "{:,.2f}".format
df_results = pd.DataFrame([joint_name, training_score, testing_score])
df_results = df_results.transpose()
df_results = df_results.rename(columns={0:'Joint',1:'Training Score',2:'Testing Score'}).sort_values(by='Testing Score',ascending=False)
df_results

feature_map_dict = {
0 : "amplitude",
1 : "percent_beyond_1_std",
2 : "maximum",
3 : "max_slope",
4 : "median",
5 : "median_absolute_deviation",
6 : "percent_close_to_median",
7 : "minimum",
8 : "skew",
9 : "std",
10 : "weighted_average"
}


#Create a dict and df of important features
important_features_data_dict = {index :feature_importance[index]['importances_mean']   for index, _ in enumerate(joint_name)}
important_features_data_df = pd.DataFrame.from_dict(important_features_data_dict)

#Temp dict of Joint names
temp_dict = { index : value  for index, value in enumerate(joint_name)}

important_features_data_df= important_features_data_df.rename(temp_dict, axis=1).rename(feature_map_dict, axis=0)
important_features_data_df

"""### Plot Important Features per Joint"""

def plot_major_JA_feature_importance_as_array_of_subplots(df, plot_name):
    fig, axs = plt.subplots(2, 4, figsize=(24,8), constrained_layout=True)

    for ax, val in zip(axs.flat, df.columns):

        ax.set_title(f'{val}')
        ax.set_xlabel('Permutation Importance', fontsize=10)

        sorted_series = df[val].sort_values(ascending=True)
        # sorted_series = sorted_series.head(5)
        ax.barh(sorted_series.index, sorted_series, align='center')

    fig.suptitle(plot_name, fontsize=25)

plot_major_JA_feature_importance_as_array_of_subplots(important_features_data_df, 'Feature Importance per Joint')

"""### Plot Train-Test Scores"""

X_axis = np.arange(len(df_results))
  
plt.bar(X_axis - 0.2, df_results['Training Score'], 0.4, label = 'Training Score')
plt.bar(X_axis + 0.2, df_results['Testing Score'], 0.4, label = 'Testing Score')
  
plt.xticks(X_axis, df_results['Joint'])
plt.xlabel("Joints")
plt.ylabel("Score")
plt.title("Train-Test Scores")
plt.legend()
plt.show()

"""### Save Models for each Joint"""

import pickle

MODEL_SAVE_PATH_ROOT = '/content/gdrive/MyDrive/ColabNotebooks/BiomechanicsAnalysis/___WORKOUTS/models/xgb'

for joint in joint_model.keys():

    json_save_path = os.path.join(MODEL_SAVE_PATH_ROOT, f"{joint}_model.json")
    pkle_save_path = os.path.join(MODEL_SAVE_PATH_ROOT, f"{joint}_model.pkl")


    joint_model[joint].save_model(json_save_path)   
    pickle.dump(joint_model[joint], open(pkle_save_path, "wb"))